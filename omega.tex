
# Introduction

## Importance of scheduling

Scheduling is planning the execution of a
set of computations that we'll call jobs in an execution environment
with a limited amount of resources. Initially scheduling was studied
in the context of operating systems, however more recently with the
popularization (?) of datacenters a different range of scheduling
techniques are being applied for the allocation of resources in
clusters. 

Clusters can run both batch jobs and long running jobs (services), two
types of jobs that represent two completely different requirements.
The former requires most of the resources in a cluster however most of
the jobs executed in modern clusters (google, yahoo references) are
batch jobs with short timespan.

One can measure the good use of a cluster by considering if the
quality constraints required by services are satisfied, the execution
time of batch jobs and the percentage of cluster utilization. The
quality of the scheduling algorithm used for a cluster affects all
these metrics so having a good scheduling algorithm has a greate
impact in terms of costs and quality of the services provided.

## Difficulties (Problematica) with cluster scheduling

However the design of a cluster scheduler is not a straightforward
task since it has many (sometimes contradictory) requirements.

The kind of applications that we are going to call frameworks
that are executed in a cluster can be quite
heterogenous. For example, the execution of Hadoop [ref]
jobs has different requirements than the execution of MPI [ref]
jobs. These frameworks execute differently but most of the
time they need to access the same data. Given that replicating the
data and the clusters is an extremely expensive alternative, frameworks
need to be executed in the same cluster that needs to be able to adapt
to the specific needs of every application while avoiding conflicts in
the assignment of resources.

Schedulers need to be adapted to execute both batch jobs and services
two kind of jobs that as we have seen have completely different
requirements that sometimes conflict. Services need to be highly
available [ref] and provide low latency demanding most of the
resources of a cluster while batch jobs are composed of thousands of small
tasks that can be affected by small delays in the scheduler and that
operate over data stored in the cluster and should
preferably be executed in the node where the data is kept.

High availability is a requirement applicable to the schedulers
themselves whose downtime affects the execution of any job to be
scheduled, but also low latency is required as we have seen in the
case of the thousand of tasks that usually conform a batch
job. Availability and low latency become harder requirements as the
cluster grows, increasing the pressure on the scheduler that becomes
more prone to failure and to delays.

## Solutions for coordinated cluster scheduling

In the following section we will explain some of the existing models
of cluster schedulers considering their advantages and disadvantages
having into account the criteria introduced in the previous
section. Finally we will introduce the model omega [ref] proposed by
engineers at google that aspires to overcome most of the
limitations of the other approaches.

### Centralized scheduler

The first schedulers were designed following a monolithic architecture
with a central scheduler taking care of the distributing the execution
of jobs across the nodes of the cluster. This design has, at least in
the beginning, the advantage of being simple as well as having
knowledge about the whole state of the cluster, what makes it capable
of taking really good scheduling decissions specially when all jobs
have the same scheduling requirements.

However this design that is simple and capable of taking good
decisions in the beginning, has many problems as the size of the
cluster increases or as the jobs executed in the cluster
diversify. When the number of jobs to execute increases, the central
scheduler needs to be able to keep with the pace of jobs that are
planned. On top of that a single scheduler represents a single point
of failure. Parallelization can atenuate both problems but just as the
cost of adding complexity and introducing the need for synchronization
between the schedulers.

But above all, for a centralized scheduler is difficult to adapt
adequately to the heteroigenity of jobs that is executed in today's
clusters [ref to google's trace]. Trying to define different
scheduling paths for the different types of workloads is difficult
because it requires algorithms to identify the type of a jobs, comes
at the expense of the scheduler's complexity and requires modifying
the code of the scheduler when a new type of workload appears.

Pro:

- Control over the whole cluster (capable of taking good scheduling decisions)
- Simplest solution when there's only one framework running in the cluster

Problems:

- Single point of failure
- Doesn't scale (bottleneck)
- Can't adapt to new frameworks with new policies & needs
- Complexity

## Static partitioning

(Cluster where alll frameworks have access to the data but they have a
limited amount of resources that are available and that is decided
once and doesn't change during the execution of jobs:

Pros:

- Simple
- Every framework can use their specific scheduler that is specialized
- There's no possibility of conflicts between frameworks when
  accessing the resources

Cons:

- Difficult to decide what is the right partition of the resources
between frameworks
- Non-optimal utilization of the resources of the cluster, for example
if one of the frameworks is idle other framework cannot use those
available resources since the repartition is done statically

### Two-Level scheduler

Better explanation of mesos:

Mesos provides a solution to the problem of scheduling that is based
on the fact that defining an scheduler to match the constraints of different
frameworks is difficult since the needs of these frameworks can be
extremely heterogenous. To avoid this problem mesos is a platform for
resource sharing where a master runs a set of frameworks. The master
offers the posibility to use the available resources to the frameworks
through offers that the frameworks can accept or reject considering if
the offered resources match the expectations of the framework. When
offers are accepted the scheduler of the framework provides
information about what tasks it want to run and what resources it will
take and the mesos master will run the tasks on the slaves and
continue to offer the available resources to the schedulers of the
other frameworks available in the cluster.

The partition of resources between frameworks is left to the master
that implements two simple policies, one based on fair resource
sharing (ref) and another on strict priorities.

(add diagram over how mesos works?)


Pros:

- Really simple central scheduler (master in Mesos terms) that has
  failover 
- Flexible: every framework decides how it schedules its tasks, the
master only offers resources following fairness (explain what fairness is?)
- Through offer rejection, frameworks can achieve high data locality
only accepting offers that correspond to nodes where the data is
stored. To avoid waiting indefinitively a technique called delayed
scheduling (ref) has proved to achieve high data locality without
sacrificing job execution time benefiting from the fact that tasks in
data processing frameworks like hadoop or MPI are short so resources
are freed frequently.
- Scalability



Glossary

- Frameworks (distributed application)
- Job
- Task
- Data locality
