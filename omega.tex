
\documentclass{svjour3}                     % onecolumn (standard format)
\usepackage[english, activeacute]{babel} %Definir idioma espaÃ±ol
\usepackage[utf8]{inputenc} %Codificacion utf-8
\usepackage[autostyle]{csquotes}
\usepackage[backend=biber]{biblatex}
\usepackage[T1]{fontenc}
\addbibresource{omega-article.bib}
\addbibresource{other.bib}

\begin{document}

\title{  Omega: flexible, scalable schedulers for large computer clusters  }

%\titlerunning{Short form of title}        % if too long for running head

\author{ Inigo Mediavilla }
\institute{ UPMC Master STL \at
              \email{imediava@gmail.com}           %  \\
}

\date{\today}

\maketitle

\begin{abstract}
This article describes the scheduler called Omega developed by
engineers at Google. It starts by providing an introduction to the
main problems that the design of a scheduler has to face, mainly the
diversity of scheduling strategies that it needs to support
while remaining capable of taking quick scheduling decisions. Then it
provides a description of the main alternatives that have been
proposed to this problems. Finally it explains the solution advocated
by Omega, describing the shared-state approach model adopted and
providing the results of a set of simulations carried out over an
implementation of Omega.
\keywords{ Scheduling \and Resource management \and Cluster }
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}

\subsection{Importance of scheduling}

Scheduling is planning the execution of a set of computations that
are called jobs in an execution environment with a limited amount of
resources. Initially scheduling was studied in the context of
operating systems but more recently with the popularization 
of datacenters a different range of scheduling techniques are being
applied for the allocation of clusters' resources.

Clusters can run both batch jobs and long running jobs (services), two
types of jobs that represent two completely different requirements.
The former requires most of the resources in a cluster and can have
complex placement requirements. The latter concentrates most of the scheduling
effort with most jobs being batch jobs (> 80\%) according to the publicly
released traces of the clusters of Yahoo \cite{parashar_10th_2010}, Google \cite{mishra_towards_2010} and Facebook \cite{Chen:EECS-2012-17}.

The good use of a cluster can be measured by considering if the
quality constraints required by services are satisfied, the execution
time of batch jobs and the percentage of cluster utilization. The
quality of the scheduling algorithm used for a cluster affects all
these metrics for that reason having a good scheduling algorithm has a big
impact in the costs and the quality of the services provided.

\subsection{Difficulties (Problematica) with cluster scheduling}

However the design of a cluster scheduler is not a straightforward
task since it has many (sometimes contradictory) requirements.

From now on we will call the applications that are executed in a
cluster frameworks.  The number of frameworks for a given cluster can
be high and the requirements for every one of them completely
different. For example, service jobs needs are orthogonal to batch
jobs needs and even map-reduce \cite{dean_mapreduce:_2008} jobs have
different constraints than MPI \cite{gabriel04:_open_mpi} jobs despite
both being used to basically process and transform data.

These frameworks adopt different strategies to place the
execution of their tasks in the cluster but most of the
time they need to access the same data. Given that replicating the
data in independent clusters is an extremely expensive alternative, frameworks
need to be executed in the same cluster that needs to be able to adapt
to the specific needs of every application while avoiding conflicts in
the assignment of resources.

Schedulers need to be adapted to execute both batch jobs and services
two kind of jobs that as we have seen have completely different
requirements that sometimes conflict. Services need to be highly
available and provide low latency demanding most of the resources of a
cluster while batch jobs are composed of thousands of small tasks that
can be affected by small delays in the scheduler and that operate over
data stored in the cluster and should preferably be executed in the
node where the data is kept but that provide a certain degree of
flexibility to the scheduler because they often allow preemption.

High availability is a requirement applicable to the schedulers
themselves whose downtime affects the execution of any job to be
scheduled, but also low latency is required as we have seen in the
case of the thousand of tasks that usually conform a batch
job. Availability and low latency become harder requirements as the
cluster grows, increasing the pressure on the scheduler that becomes
more prone to failure and to delays.

\subsection{Solutions for coordinated cluster scheduling}

In the following section we will explain some of the existing models
of cluster schedulers considering their advantages and disadvantages
having into account the criteria introduced in the previous
section. Finally we will introduce the model Omega \cite{41684}
proposed by engineers at google that aims to overcome most of the
limitations of the other approaches.

\subsubsection{Centralized scheduler}

The first schedulers were designed following a monolithic architecture
with a central scheduler taking care of distributing the execution
of jobs across the nodes of the cluster. This design has, at least in
the beginning, the advantage of being simple since there's only one
scheduler assigning resources as well as having knowledge about the
whole state of the cluster, what makes it capable of taking really
good scheduling decisions specially when all jobs have the same
scheduling requirements. Besides, can provide a good
utilization of the resources of the cluster.

However this design that is simple and capable of taking good
decisions in the beginning, has many problems as the size of the
cluster increases or as the jobs executed in the cluster
diversify. When the number of jobs to execute increases, the central
scheduler needs to be able to keep with the pace of jobs that are
planned. On top of that a single scheduler represents a single point
of failure. Parallelization can atenuate both problems but just at the
expense of adding complexity and introducing the need for synchronization
between the schedulers.

Above all, for a centralized scheduler is difficult to adapt
adequately to the heterogeinity of jobs that is executed in today's
clusters \cite{37201}. Trying to define different
scheduling paths for the different types of workloads is difficult
because it requires algorithms to identify the type of a job, and comes
at the expense of the scheduler's complexity. Some schedulers try to
support different policies by implement really complex algorithms
involving different weighting factors that provide rough
approximations of the objectives of every kind of workload. Others
like google's run different scheduling logic for every kind of
workload but it usually implies that users of the scheduler need to
have a really good understanding of the internals of the scheduling
algorithm to be able to optimize the execution of jobs. In both cases
the solution implies an additional layer of complexity to an already
difficult task.

Besides this model requires modifying the code of the scheduler every
time a new type of workload appears. 

Advantages:

\begin{itemize}
  \item Control over the whole cluster (capable of taking good scheduling decisions)
  \item Good cluster utilization
  \item Simplest solution when there's only one framework running in the cluster
\end{itemize}

Disadvantages:

\begin{itemize}
  \item Single point of failure
  \item Doesn't scale (bottleneck)
  \item Can't adapt to new frameworks with new policies and needs
  \item Complexity
  \item It doesn't scale well when the cluster grows
\end{itemize}

\subsection{Static partitioning}

To overcome the limitations that the previous model had when dealing
with clusters where the categories of jobs have different scheduling
needs, static partitioning allows to split the resources of the cluster
in as many partitions as the number of categories of jobs that we want to
treat independently. This strategy is attractive because it allows to
have an optimized dedicated scheduler for every category while being
relatively simple since every scheduler manages its portion of the
cluster without any risk of conflicts with the others. It was
for a while the model chosen by companies like Yahoo.

Nonetheless this strategy's drawback is that since the different 
categories run in different, isolated portions of the cluster the
resources of the cluster are squandered since there exists no way for
a category to use the resources that another category is not utilizing.

Advantages:

\begin{itemize}
    \item Simple
    \item Every framework can use their specific scheduler that is specialized
    \item There's no possibility of conflicts between frameworks when
      accessing the resources
\end{itemize}

Disadvantages:

\begin{itemize}
  \item Difficult to decide what is the right partition of the resources
between frameworks
  \item Non-optimal utilization of the resources of the cluster, for example
if one of the frameworks is idle other framework cannot use those
available resources since the repartition is done statically
\end{itemize}

\subsection{Two-Level scheduler}

Two level schedulers give applications, also called
frameworks, the ability to schedule their tasks independently while
having access to all the resources available in the cluster. This
is achieved with an architecture that is based in a master that manages
the cluster's resources and offers them to the frameworks who can accept 
the offers, notifying the master of the tasks they want to run and the
resources they want to run them on, or reject them.

The resources of the cluster are provided by the slaves that register to 
the master at startup providing information about the amount of memory, cpu
and network bandwith that they put at master's disposal.  

This way frameworks can satisfy their needs (for example data locality \cite{chung_maximizing_2006} ) by 
rejecting the offers that don't interest them and accepting those who do, thus
ensuring per framework needs despite a centralized resource manager. 

However, problems start when the interests of two frameworks conflict 
(for example two frameworks want to run their jobs in the same slave because 
they both access the same data). In the case of data processing jobs where tasks
don't take long to execute a technique called delay scheduling \cite{zaharia_delay_2010} has ensured to
provide good data locality for the tasks without delaying the execution of the whole
job. Nonetheless, when two competing frameworks use resource hoarding to incrementally 
adcquire resources before they start a job deadlocks can emerge.

At the cluster level, global policies are enforced by the way that the master 
distributes the offers to the frameworks. A cluster manager like Mesos \cite{Hindman10mesos:a} provides two
policies one based on fairness \cite{AjtaiANRSW1998} and another on strict priorities, custom policies can
also be implemented.

(It is still a weak way to enforce global policies, if we were to define a complex
policy it would stil be difficult to implement it like that)

Failover on the master is provided with machines that run as backups of the master and
are capable to restore the master's state when it falls. Schedulers and slaves can find
the new master to register to with centralized configuration services like Zookeeper \cite{_apache_????}.

Partitioning of the resources of the cluster is provided thanks to linux containers \cite{_linux_????}
an isolation technique that allows to run processes in an allocated subset of the 
resources of the machine without interferences from other processes. 

An optimization to the model of resource offers is the possibility for the frameworks
to define filters that describe the kinds of offers that a framework will not be interested
in, this way avoiding innecesary exchanges between the slaves and the masters.

\subsubsection{Execution process}

A simple example where we have only one framework that wants to run a 
job formed by a single task will serve to explain all the steps that happen
before a computation can take place in the cluster:

\begin{enumerate}
\item Master starts. One of the candidates to master is selected as the master and 
the others remain as backup masters that will take the place of the current leader in
case it fails. The leader will start listening to the registration of slaves and 
frameworks.
\item Slaves start. They find the current master through a distributed configuration 
service and they register themselves by notifying the resources the put at the disposal
of the cluster.
\item The framework is launched. Our framework is started with the idea of executing 
the job. To do that, it first registers to the master and starts listening to resource
offers.
\item Since our framework is the only one in the cluster, the master makes an offer with
all the resources available to our framework.
\item The framework receives the offer and accepts it. It does so by responding to the
offer with a list of the tasks that it wants to run specifying for every task: how to 
execute the task and what resources will be dedicated to the execution of the task.
The response will only be valid if the resources assigned to all the tasks are a subset
of the resources offered by the master.
\item When the master receives the proposition to run the tasks. It allocates the requested
resources inside containers in the slaves specified in the response and it launches the
process to compute the task. To be able to run the tasks the slave obviously needs to 
have access to the binaries to run the process what is usually done either making the
binaries accessible to the whole cluster through a distributed filesystem like HDFS [ref]
or S3[ref] or by ensuring manually that the slaves have all the binaries in a public
accessible folder.
\item Once the computation has finished the slave notifies the master that the resources
are newly available so the master can continue offering them to the frameworks.
\end{enumerate}


(add diagram over how mesos works?)

Advantages:

\begin{itemize}
  \item
  Really simple central scheduler (master in Mesos terms) that has
  failover 
 \item
  Flexible: every framework decides how it schedules its tasks, the
  master only offers resources following fairness (explain what fairness is?)
 \item
  Through offer rejection, frameworks can achieve high data locality
  only accepting offers that correspond to nodes where the data is
  stored. To avoid waiting indefinitively a technique called delayed
  scheduling (ref) has proved to achieve high data locality without
  sacrificing job execution time benefiting from the fact that tasks in
  data processing frameworks like hadoop or MPI are short so resources
  are freed frequently.
 \item Scalability
 \item Specialized for jobs with high chunk and where schedulers don't take
too long. With this kind of job, resources are freed frequently and
every framework gets its faire share of the cluster.
\end{itemize}

Disadvantages:

\begin{itemize}
 \item Difficult to place picky jobs, for example to do gang scheduling
frameworks in Mesos need to do resource hoarding what can lead to
deadlocks
 \item Pessimistic locking, while a framework is offered some resources it
holds the lock for the resource for the time the offer is
available. For example if a framework holds the resources or takes the
time to respond all other frameworks that are actually ready to accept
the offer and execute the job will have to wait for the first
framework to respond to the offer.
 \item Framework scheduler written in an indirect style (adapt to the interface, 
  take actions only when the resources are offered in an indirect away unlike
  the direct style given by optimistic locking
\end{itemize}

\subsection{Optimistic locking}

Another approach to cluster scheduling that tries to overcome some of
the limitations of the pessimistic locking of two-level schedulers is
the optimistic locking approach proposed by google's Omega
scheduler.

The idea behind Omega is to make the state of the cluster
available to every scheduler through a data structure that its called
cell state. When one of the frameworks wants to run a job in the
cluster it tries to commit a lock for the resources needed to run the
job's tasks. If the task has already been taken by another framework
since the last update state, the framework receives the updated
version of the cluster's state and it can retry a commit knowing the
new state of the cluster. With this approach every scheduler can have
an overall view of the cluster that what can allow them to make better
scheduling decisions while having a mediation mechanism when two
frameworks want to commit for the same resources.

\subsubsection{Optimizations}

The performance of this solution depends completely on how often
conflicts happen and how this conflicts are resolved. An initial
implementation of shared state used sequence numbers to detect
conflicts. Whenever a machine was touched by a framework the sequence
number for that machine was incremented and if another framework
wanted to use the same machine it would detect the increment in the
sequence number and signal a conflict. This implementation didn't
performed well, showing a great deviation when compared with the best
possible situation where the jobs run without conflicts.

The main reason was that this mechanism didn't account for the level
of utilization of the machine, and thus a machine couldn't run two
frameworks at the same time even if the resources of the machine could
hold both frameworks running concurrently. Besides when scheduling
jobs, a job was refused when one of the resource allocations run into
a conflict even if all the other tasks were well allocated,
duplicating the amount of scheduling work to do and delaying the
execution of tasks. To overcome this limitation a model that dealt
with fine-grained resource allocation, where allocating could be done
only in a subset of the machine's resources was introduced. This
optimization added to the possibility to do incremental scheduling of
tasks, having to retry only the conflicting tasks when the scheduling
of a job run into a conflict gave Omega the performance expected.

\subsubsection{Simulation}

The more challenging problem that Omega wanted to solve was how
to build an scheduler that could keep the pace when scheduling batch
jobs and service jobs even if the service jobs took longer to
schedule. To analyze the performance of the existing solutions and
compare them with Omega the designers of the shared-state based
scheduler built a lightweight simulator to test the behaviour of
schedulers agains variations in the decision time for service jobs.

In the simulator the decision time (time to schedule a job) was
calculated as

t\_{decis} = t\_{job} + t\_{task} x number of task

where t\_{job} is an estimation of the extra time require to schedule
every job and t\_{task} an estimation of the average time needed to
schedule a task. Both t\_{job} and t\_{task} where taken as
overestimations of the values obtained from google cluster's traces
[ref] to stress test Omega a situation even more difficult than that
Omega would find when running in a production cluster.

Modifying the parameter t\_{job} allowed to simulate a longer decision
time for service jobs and this way allowing to perceive the effect it
has on the different scheduling models. The effects were measured
through two metrics: job wait time and scheduler busyness. Job wait
time represents the time that happens between the first job submission
and the first scheduling attempt. Scheduler busynes is the fraction of
the time that the scheduler is busy making scheduling decisions.

When incrementing t\_{job} for services in the monolithic single-path
scheduler both job wait time and busyness increased linearly till the
scheduler reached saturation and couldn't keep with the number of jobs
that arrived. Those were the results expected since in this kind of
scheduler there is no parallelism and batch jobs and service jobs share
the same scheduling logic, so a delay in scheduling services affects
immediately batch jobs.

In the case of the multipath monolithic scheduler where there is
a fast path for scheduling batch jobs, job wait time and scheduler
busyness are reduced but due to the lack of parallelism the queue can
still be blocked by a service, so there is still head of line blocking.

The test for the two level scheduler (Mesos) defined two independent
schedulers, one for service jobs and another for batch jobs. In the
test decision time for batch jobs was kept stable and that of service
jobs varied. Mesos resource allocation algorithm based on DFR [ref]
offers all available resources to a framework and only what is left to
the following. This algorithm works really well when tasks are short
and resources are freed frequently. However it didn't work well in the
simulation because the services that had long decision times locked
their resources and batch jobs couldn't schedule their tasks.

\section{Summary}

As we saw in the introduction, due to the diversity of processes that
are executed in a cluster, the need to share data between applications
and the scalability requirements of clusters, designing a scheduler is
a difficult task.

After a comparison of the different models of schedulers a group of
people at Google came up with the idea of Omega, a cluster scheduler
based on a shared-state approach that provides a good compromise
between simplicity and the fulfillment of the scheduling requirements
of Google's clusters. Omega can provide a good level of cluster
utilization due to every framework knowing the current state of
available resources for the whole cluster. It is well adapted for a
variety of framework's scheduling needs since every framework takes
care of doing his own scheduling and Omega only exposes the
availability of resources and protects against allocation
conflicts. It performs as well as the multi-path monolithic scheduler
as it has been proven by the simulations carried out by its designers,
even as the decision time of service jobs grows and the possibility of
conflicts increases. Besides, it allows frameworks to dynamically
increase or decrease the resource consumption, for example to speed up
the execution of jobs when the cluster is under low usage.

However even if Omega fits really well the context of Google cluster's
it has its limitations. It doesn't work well when many jobs compete
frequently for access to the same resource (e.g many batch jobs want to run
on the same cluster node) or when cluster utilization is really high,
since the possibility for conflicts increases and Omega doesn't
provide a centralized way to coordinate the scheduling of different
frameworks. This lack of a centralized intelligence makes it difficult
to enforce global policies in the cluster or control \emph{rogue}
frameworks. For the moment Omega only provides global policies through
a simple mechanism for dealing with priorities and through post-facto
monitoring but this is one of the biggest axes of improvement for the
shared-state approach.

\section{Bibliography}


\printbibliography


\end{document}
