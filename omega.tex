
# Introduction

## Difficulties (Problematica) with cluster scheduling

Scheduling is planning the execution of a
set of computations that we'll call jobs in an execution environment
with a limited amount of resources. Initially scheduling was studied
in the context of operating systems, however more recently with the
popularization (?) of datacenters a different range of scheduling
techniques are being applied for the allocation of resources in
clusters. 

Clusters can run both batch jobs and long running jobs (services), two
types of jobs that represent two completely different requirements.
The former requires most of the resources in a cluster however most of
the jobs executed in modern clusters (google, yahoo references) are
batch jobs with short timespan.

One can measure the good use of a cluster by considering if the
quality constraints required by services are satisfied, the execution
time of batch jobs and the percentage of cluster utilization. The
quality of the scheduling algorithm used for a cluster affects all of
this metrics so considering the costs involved in running a cluster in
a datacenter and the impact in bussiness terms of having the
appropriate quality of service for jobs is obvious the importance of
having a good scheduling algorithm. 


## Factors to consider when doing scheduling

- Independent frameworks with different scheduling needs (and
  currently different implementations  of the scheduling) (e.g Hadoop
  schedules differently from MPI)
- Those frameworks need to access the same data (they cannot run in
different clusters or otherwise they'll need to replicate the data)

So => Ideally there's a need to allow the different frameworks to have an
scheduling that is adapted to their specific needs while they need to
share the same cluster so they need to coordinate to be able to share
the available resources.

- Other needs:

 - Main use of a cluster: batch processes over distributed data
   (usually map-reduce but it can also be MPI) it is a really specific
   type of job that needs :
     - Data locality (so scheduling needs to be done so that jobs that
     process information that is stored in a specific node of the
     cluster can be executed in that node)
     - Many jobs, that don't take too long to execute (intensive work
     for the scheduler and that means that a minor increase in the
     delay per-task can have a big impact in the duration of jobs that
     are formed but thousands of small tasks)
 - Needs to deal with services and jobs (two different needs)
 - Needs to scale when increasing the number of machines and the
   number of jobs in the cluster
 - Ideally the scheduler needs to be simple and should not need to be
   modified when new frameworks with different needs are added

## Solutions for coordinated cluster scheduling

### Centralized scheduler

Pro:

- Control over the whole cluster
- Simplest solution when there's only one framework running in the cluster

Problems:

- Complexity
- Can't adapt to new frameworks with new policies & needs
- Doesn't scale

## Static partitioning

(Cluster where alll frameworks have access to the data but they have a
limited amount of resources that are available and that is decided
once and doesn't change during the execution of jobs:

Pros:

- Simple
- Every framework can use their specific scheduler that is specialized
- There's no possibility of conflicts between frameworks when
  accessing the resources

Cons:

- Difficult to decide what is the right partition of the resources
between frameworks
- Non-optimal utilization of the resources of the cluster, for example
if one of the frameworks is idle other framework cannot use those
available resources since the repartition is done statically

### Two-Level scheduler

Better explanation of mesos:

Mesos provides a solution to the problem of scheduling that is based
on the fact that defining an scheduler to match the constraints of different
frameworks is difficult since the needs of these frameworks can be
extremely heterogenous. To avoid this problem mesos is a platform for
resource sharing where a master runs a set of frameworks. The master
offers the posibility to use the available resources to the frameworks
through offers that the frameworks can accept or reject considering if
the offered resources match the expectations of the framework. When
offers are accepted the scheduler of the framework provides
information about what tasks it want to run and what resources it will
take and the mesos master will run the tasks on the slaves and
continue to offer the available resources to the schedulers of the
other frameworks available in the cluster.

The partition of resources between frameworks is left to the master
that implements two simple policies, one based on fair resource
sharing (ref) and another on strict priorities.

(add diagram over how mesos works?)


Pros:

- Really simple central scheduler (master in Mesos terms) that has
  failover 
- Flexible: every framework decides how it schedules its tasks, the
master only offers resources following fairness (explain what fairness is?)
- Through offer rejection, frameworks can achieve high data locality
only accepting offers that correspond to nodes where the data is
stored. To avoid waiting indefinitively a technique called delayed
scheduling (ref) has proved to achieve high data locality without
sacrificing job execution time benefiting from the fact that tasks in
data processing frameworks like hadoop or MPI are short so resources
are freed frequently.
- Scalability



Glossary

- Frameworks (distributed application)
- Job
- Task
- Data locality
