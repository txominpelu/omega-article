
\documentclass{article}                     % onecolumn (standard format)
\usepackage[english, activeacute]{babel} %Definir idioma espa√±ol
\usepackage[utf8]{inputenc} %Codificacion utf-8
\usepackage[autostyle]{csquotes}
\usepackage[backend=biber]{biblatex}
\addbibresource{omega-article.bib}
\addbibresource{other.bib}

\begin{document}

\title{  Omega: flexible, scalable schedulers for large computer clusters  }

%\titlerunning{Short form of title}        % if too long for running head

\author{ Inigo Mediavilla }
%\institute{ UPMC Master STL \at
%              \email{imediava@gmail.com}           %  \\
%}

\date{\today}

\maketitle

\begin{abstract}

Scheduling has a great impact in the well functioning of a cluster in
terms of quality of service and running costs.  Unfortunately
designing a cluster scheduler is a difficult task, mainly due to the
diversity of scheduling strategies that the scheduler needs to support
while remaining capable of taking quick scheduling decisions.

This article describes the scheduler called Omega developed by
engineers at Google that provides a simple strategy based on a
shared-state approach to allow frameworks to schedule their jobs
independently and mediation to resolve conflicting demands.

The proposed model is described, compared to the existing solutions
and some tips for an efficient implementation provided. In the end the
article shows the results of tests that compare the performance of a
lightweight simulator against the different models showing that Omega
outperforms the other models in the conditions Google's cluster
traces. It also proposes a MapReduce scheduler to run in Omega that
benefits from the transparence of Omega and manages to dynamically
adapt its allocation of workers to the conditions of the cluster
increasing significantly the speed of the standard MapReduce scheduler.


%\keywords{ Scheduling \and Resource management \and Cluster }
% \PACS{PACS code1 \and PACS code2 \and more}
% \subclass{MSC code1 \and MSC code2 \and more}
\end{abstract}

\section{Introduction}

\subsection{Importance of scheduling}

Scheduling is planning the execution of a set of computations that
are called jobs in an execution environment with a limited amount of
resources.

% Initially scheduling was studied in the context of
% operating systems but more recently with the popularization 
% of datacenters a different range of scheduling techniques are being
% applied for the allocation of clusters' resources.


Clusters can run both batch jobs and long running jobs (services), two
types of jobs that represent two completely different requirements.
The former requires most of the resources in a cluster and can have
complex placement requirements. The latter concentrates most of the scheduling
effort with most jobs being batch jobs (\textgreater 80\%) according to the publicly
released traces of the clusters of Yahoo \cite{parashar_10th_2010}, Google \cite{mishra_towards_2010} and Facebook \cite{Chen:EECS-2012-17}.

The good use of a cluster can be measured by considering if the
quality constraints required by services are satisfied, the execution
time of batch jobs and the percentage of cluster utilization. The
quality of the scheduling algorithm used for a cluster affects all
these metrics, for that reason having a good scheduling algorithm has a big
impact in the costs and the quality of the services provided.

\subsection{Difficulties with cluster scheduling}

The design of a cluster scheduler is not a straightforward
task since it has many, sometimes contradictory, requirements.

From now on we will call the applications that are executed in a
cluster frameworks.  The number of frameworks for a given cluster can
be high and the requirements for every one of them completely
different. For example, service jobs needs are orthogonal to batch
jobs needs and even MapReduce \cite{dean_mapreduce:_2008} jobs have
different constraints than MPI \cite{gabriel04:_open_mpi} jobs despite
both being used to process and transform data.

These frameworks adopt different strategies to place the
execution of their tasks in the cluster but most of the
time they need to access the same data. Given that replicating the
data in independent clusters is an extremely expensive alternative, frameworks
need to be executed in the same cluster whose scheduler needs to be able to adapt
to the specific needs of every application while avoiding conflicts in
the assignment of resources.

Schedulers need to be prepared to execute both batch jobs and services,
two kind of jobs that as we have seen have completely different
requirements. Services need to be highly available and provide low
latency, demanding most of the resources of a cluster. Batch jobs are
composed of thousands of small tasks so they can be affected by small
delays in the scheduler. They operate over data stored in the cluster
and should preferably be executed in the node where the data resides
but their requirements are flexible and often allow preemption.

High availability is a requirement applicable to the schedulers
themselves whose downtime affects the execution of any job to be
scheduled, but also low latency is required as we have seen in the
case of the thousand of tasks that usually conform a batch
job. Availability and low latency become harder requirements as the
cluster grows, increasing the pressure on the scheduler that becomes
more prone to failure and to delays.

\section{Solutions for coordinated cluster scheduling}

In the following section we will explain some of the existing models
of cluster schedulers considering their advantages and disadvantages
having into account the criteria introduced in the previous
section. Finally in section \ref{sec:omega} we will introduce the
model Omega \cite{41684} proposed by engineers at google that aims to
overcome many of the limitations of the other approaches.

\subsubsection{Centralized scheduler}

The first schedulers were designed following a monolithic architecture
with a central scheduler taking care of distributing the execution
of jobs across the nodes of the cluster. This design has, at least in
the beginning, the advantage of being simple since there's only one
scheduler assigning resources. The central scheduler controls the
state of the whole cluster, what makes it capable of taking really
good scheduling decisions specially when all jobs have the same
scheduling requirements. 

However this design that is simple and capable of taking good
decisions in the beginning, has many problems as the size of the
cluster increases or as the jobs executed in the cluster
diversify. When the number of jobs to execute increases, the central
scheduler needs to be able to keep with the pace of jobs that are
planned. On top of that a single scheduler represents a single point
of failure. Parallelization can attenuate both problems but just at the
expense of adding complexity and introducing the need for synchronization
between the schedulers.

Above all, for a centralized scheduler it is difficult to adapt
adequately to the heterogeneity of jobs that is executed in today's
clusters \cite{37201}. Trying to define different
scheduling paths for the different types of workloads is difficult
because it requires algorithms to identify the type of a job, and comes
at the expense of the scheduler's complexity. Some schedulers try to
support different policies by implementing really complex algorithms
involving different weighting factors that provide rough
approximations of the objectives of every kind of workload. Others
like google's run different scheduling logic for every kind of
workload but it usually implies that users of the scheduler need to
have a really good understanding of the internals of the scheduling
algorithm to be able to optimize the execution of jobs. In both cases
the solution implies an additional layer of complexity to an already
difficult task.

Besides this model requires modifying the code of the scheduler every
time a new type of workload appears.  \\

Advantages:

\begin{itemize}\itemsep4pt
  \item Control over the whole cluster. The scheduler is capable of taking good scheduling decisions.
  \item Good cluster utilization.
  \item Simplest solution when there's only one framework running in
    the cluster. \\
\end{itemize}

Disadvantages:

\begin{itemize}\itemsep4pt
  \item Single point of failure.
  \item Doesn't scale. The central scheduler is a bottleneck and
    slower jobs block other jobs that are faster to schedule.
  \item Can't adapt to new frameworks with new policies and needs.
  \item Complexity.
\end{itemize}

\subsection{Static partitioning}

To overcome the limitations that the previous model had when dealing
with clusters where the are many kinds of jobs with different scheduling
needs, static partitioning allows to split the resources of the cluster
in as many partitions as the kinds of jobs that we want to
treat independently. This strategy is attractive because it allows to
have an optimized dedicated scheduler for every kind while being
relatively simple since every scheduler manages its portion of the
cluster without any risk of conflicts with the others. It was
for a while the model chosen by companies like Yahoo.

Nonetheless this strategy's drawback is that since the different 
categories run in different, isolated portions of the cluster the
resources of the cluster are wasted since it is not possible for
a partition to use the resources that another partition is not using. \\

Advantages:

\begin{itemize}\itemsep4pt
    \item Simple.
    \item Every framework can use their specific scheduler that is specialized.
    \item There's no possibility of conflicts between frameworks when
      accessing the resources. \\
\end{itemize}

Disadvantages:

\begin{itemize}\itemsep4pt
  \item Difficult to decide what is the right partition of the resources
between frameworks.
  \item Non-optimal utilization of the resources of the cluster. If
    one partition is idle the frameworks of other partitions cannot
    use its available resources since the repartition is done statically.
\end{itemize}

\subsection{Two-Level scheduler}

Two level schedulers give applications, also called
frameworks, the ability to schedule their tasks independently while
having access to all the resources available in the cluster. This
is achieved with an architecture that is based in a master that manages
the cluster's resources and offers them to the frameworks who can accept 
the offers, notifying the master of the tasks they want to run and the
resources they want to run them on, or reject them.

The resources of the cluster are provided by the slaves that register to 
the master at startup providing information about the amount of memory, CPU
and network bandwidth  that they put at master's disposal.  

Frameworks can satisfy their needs (for example data locality \cite{chung_maximizing_2006} ) by 
rejecting the offers that don't interest them and accepting those who do, thus
ensuring per framework needs despite a centralized resource manager. 

When the interests of two frameworks conflict, for example two
frameworks want to run their jobs in the same slave because they both
access the same data, there are techniques to ensure that all
frameworks can have their fair share of the resources they need. In
the case of data processing jobs where tasks don't take long to
execute a technique called delay scheduling \cite{zaharia_delay_2010}
manages to provide good data locality for the tasks without delaying
the execution of the whole job. Nonetheless, when two competing
frameworks use resource hoarding to incrementally acquire resources
before they start a job deadlocks can emerge.

At the cluster level, global policies are enforced by the way that the
master distributes the offers to the frameworks. A cluster manager
like Mesos \cite{Hindman10mesos:a} provides two policies one based on
fairness \cite{AjtaiANRSW1998} and another on strict priorities,
custom policies can also be implemented. However if this policies are
too simple the central scheduler of Mesos can be extended with new
policies.

Failover on the master is provided with machines that run as backups of the master and
are capable of restoring the master's state when it falls. Schedulers and slaves can find
the new master to register to with centralized configuration services like Zookeeper \cite{_apache_????}.

Partitioning of the resources of the cluster is provided thanks to Linux containers \cite{_linux_????}
an isolation technique that allows to run processes in an allocated subset of the 
resources of the machine without interferences from other processes. 

An optimization to the model of resource offers is the possibility for the frameworks
to define filters that describe the kinds of offers that a framework will not be interested
in, this way avoiding unnecessary exchanges between the slaves and the masters.

\subsubsection{Execution process}

A simple example where we have only one framework that wants to run a 
job formed by a single task will serve to explain all the steps that happen
before a computation can take place in the cluster:

\begin{enumerate}\itemsep4pt
\item Master starts. One of the candidates is selected as the master and 
the others remain as backup masters that will take the place of the current leader in
case it fails. The leader will start listening to the registration of slaves and 
frameworks.
\item The slaves start. They find the current master through a distributed configuration 
service and they register themselves by notifying the resources the put at the disposal
of the cluster.
\item The framework is launched. To execute its job the framework
  first registers to the master and starts listening to resource
  offers.
\item Since our framework is alone in the cluster, the master makes an offer with
all the resources available to our framework.
\item The framework receives the offer and accepts it. It does so by responding to the
offer with a list of the tasks that it wants to run specifying for every task how to 
execute the task and what resources will be dedicated to the execution.
The response will only be valid if the resources assigned to all the tasks are a subset
of the resources offered by the master.
\item When the master receives the proposition to run the tasks. It
  allocates the requested resources inside containers in the slaves
  specified in the response and it launches the process to compute the
  task. To be able to run the tasks the slave obviously needs to have
  access to the binaries to run the process what is usually done
  either making the binaries accessible to the whole cluster through a
  distributed filesystem like the Google File System
  \cite{ghemawat_google_2003} or Amazon's S3 \cite{_aws_????} or by
  ensuring manually that the slaves have all the binaries in a publicly
  accessible folder.
\item Once the computation has finished the slave notifies the master that the resources
are available again so the master can continue offering them to the frameworks.
\end{enumerate}


%% (add diagram over how mesos works?)

Advantages:

\begin{itemize}\itemsep4pt
  \item
  Really simple central scheduler (master in Mesos terms) that has
  failover.
 \item
  Flexible: every framework decides how it schedules its tasks, the
  master only offers resources following fairness.
 \item
  Through offer rejection, frameworks can achieve high data locality
  only accepting offers that correspond to nodes where the data is
  stored. To avoid waiting indefinitely a technique called delayed
  scheduling \cite{zaharia_delay_2010} has proved to achieve high data locality without
  sacrificing job execution time benefiting from the fact that tasks in
  data processing frameworks like Hadoop or MPI are short so resources
  are freed frequently.
 \item Scalability.
 \item Specialized for jobs with high chunk and where schedulers don't take
too long. With this kind of job, resources are freed frequently and
every framework gets its fair share of the cluster.
\end{itemize}

Disadvantages:

\begin{itemize}\itemsep4pt
 \item This kind of scheduling encourages frameworks to demand more
   resources than they actually need in preparation for everything
   they want to run. If a framework wants to run a list of tasks in
   many steps it will demand resources for the whole process even if
   many resources will be idle during most steps.
 \item Difficult to place picky jobs, for example to do gang scheduling
frameworks in Mesos need to do resource hoarding what can lead to
deadlocks.
 \item When a framework is offered some resources it
holds the lock for the resource for the time the offer is
available. If a framework hoards the resources or takes a long
time to respond all other frameworks that are actually ready to accept
the offer and execute their jobs will have to wait for the first
framework to respond to the offer.

For example in clusters that run service jobs or jobs with complex
placement constraints that take long time to schedule the performance
of two-level schedulers suffers.

 \item Framework schedulers are written in an indirect style. They
  take actions only when the resources are offered. 
\end{itemize}

\section{Omega}
\label{sec:omega}

Another approach to cluster scheduling that tries to overcome some of
the limitations of the pessimistic locking of two-level schedulers is
the optimistic locking approach proposed by Google's Omega
scheduler.

The idea behind Omega is to make the state of the cluster available to
every scheduler through a data structure that its called cell
state. When one of the frameworks wants to run a job in the cluster it
tries to commit a lock for the resources needed to run the job's
tasks. If the resource has already been taken by another framework
since the last update, the framework receives the updated version of
the cluster and it can retry a commit of available resources. With
this approach every scheduler has an overall view of the cluster what
allows them to make better scheduling decisions while having a
mediation mechanism when two frameworks want to commit for the same
resources.

\subsection{Optimizations}

The performance of this solution depends completely on how often
conflicts happen and how this conflicts are resolved. An initial
implementation of shared state used sequence numbers to detect
conflicts. Whenever a machine was touched by a framework the sequence
number for that machine was incremented and if another framework
wanted to use the same machine it would detect the increment in the
sequence number and signal a conflict. This implementation didn't
performed well, showing a great deviation when compared with the best
possible situation where the jobs run without conflicts.

The main reason was that this mechanism didn't consider that a machine
can have available resources even if a framework is already running
tasks on it. Besides when scheduling jobs, a job was refused when one
of the resource allocations for a particular task run into a
conflict even if all the other tasks were well allocated, duplicating
the amount of scheduling work to do and delaying the execution of
tasks. To overcome the first limitation a model that dealt with
fine-grained resource allocation, where allocation could be done only
in a subset of the machine's resources was introduced. Another
optimization added the possibility to do incremental scheduling of
tasks as well, having to retry only the unscheduled tasks after a
conflict.

After this two optimizations were implemented a bigger range
of tests were run that allowed to compare the behaviour of Omega, with
this more performant setup, and the rest of the existing scheduling
models. The section \ref{sec:simulation} shows the results of this
experiments and the place that Omega takes in relation to its pairs.

\subsection{Simulation}
\label{sec:simulation}

The more challenging problem that Omega wanted to solve was how
to build an scheduler that could keep the pace when scheduling batch
jobs and service jobs even if the service jobs took longer to
schedule. To analyze the performance of the existing solutions and
compare them with Omega, the designers of the shared-state based
scheduler built a lightweight simulator to test the behavior of
schedulers against variations in the decision time for service jobs.

In the simulator the decision time (time to schedule a job) was
calculated as
$t_{decis}\ =\ t_{job}\ +\ t_{task}\ \times$ \emph{number of tasks}
where $t_{job}$ is an estimation of the extra time required to schedule
every job and $t_{task}$ an estimation of the average time needed to
schedule a task. Both $t_{job}$ and $t_{task}$ where taken as
overestimations of the values obtained from Google cluster's traces
\cite{37201}. 

Modifying the parameter $t_{job}$ allowed to simulate a longer
decision time for service jobs to perceive the effect this has on the
different scheduling models. The effects were measured through two
metrics: job wait time and scheduler busyness. Job wait time
represents the time that happens between the first job submission and
the first scheduling attempt. Scheduler busyness is the fraction of
the time that the scheduler is busy making scheduling decisions.

When incrementing $t_{job}$ for services in the monolithic single-path
scheduler both job wait time and busyness increased linearly till the
scheduler reached saturation and couldn't keep with the number of jobs
that arrived. Those were the results expected since in this kind of
scheduler there is no parallelism and batch jobs and service jobs share
the same scheduling logic, so a delay in scheduling services affects
immediately batch jobs.

In the case of the multi-path monolithic scheduler where there is
a fast path for scheduling batch jobs, job wait time and scheduler
busyness are reduced but due to the lack of total parallelism the queue can
still be blocked by a service, so there is still head of line blocking.

The test for the two level scheduler (Mesos) defined two independent
schedulers, one for service jobs and another for batch jobs. In the
test, decision time for batch jobs was kept stable and that of service
jobs varied. Mesos resource allocation algorithm based on using
fairness for offering resources to the frameworks offers all available
resources to a framework and only what is left to the following. This
algorithm works really well when tasks are short and resources are
freed frequently. However it didn't work well in the simulation
because the services that had long decision times locked their
resources and batch jobs couldn't schedule their tasks.

The Omega simulator performed as well as the multi-path monolithic
scheduler in the normal situation described by the traces but it it
doesn't suffer from head of line blocking because service jobs and
batch jobs do not share an scheduling path. Tests where also performed
to see the behaviour of Omega as the cluster and the interarrival rate
of jobs increases. Omega outperform all the other models in this test,
being capable of dealing with in between 2.5 and 9.5 times the
original workflow before starting failing. Other test showed that
multiple loadbalanced Omega schedulers could deal with even larger
loads, having an increased conflict rate that is compensated by a
lower per scheduler busyness. In summary the tests proved that Omega
works as well as the best model in the standard situation and is
better fitted for an increase in the load of the cluster.

\subsection{A MapReduce scheduler}

One of the benefits of the Omega scheduler is that it gives constantly
to the frameworks the knowledge of what is the current state of the
cluster. This is a knowledge that other models didn't provide or only
provided partially in the cases of two level schedulers and that
offers the possibility of implementing new scheduling techniques that
were not possible before. One of this techniques allows to modify the
behavior of a framework based on the busyness of the framework. The
engineers behind Omega defined an evolution of a MapReduce scheduler
that uses this technique and that achieves a remarkable increase in
the performance of its jobs. We will explain this evolution, how it
compares with the standard MapReduce scheduler and the conditions of
the tests that were used to evaluate its performance.

Standard MapReduce schedulers have the numbers of workers set at
submission time. To set this parameter engineers use estimations based
on data obtained by the traces of the execution of similar jobs,
approximations of the busyness of the cluster, trial and error and
intuition. However the scheduler for an Omega framework knows the
state of the cluster at all times, so it can benefit from this
knowledge to vary the number of workers and increase it when many of
the resources of the cluster are idle. This is what the authors of the
Omega paper did with their MapReduce scheduler that observes the state
of the cluster, predicts the benefits of adding more workers to
current tasks and tasks to come and they allocate more resources
according to some policy.

In the experiment the policies tested where \emph{max-parallelism},
\emph{global cap} and \emph{relative job size}. \emph{Max-parallelism}
keeps adding resources to the job as long as there is benefit from
it.\emph{Global cap} sets a maximum of busyness to the cluster and
doesn't allow to allocate more resources when this limit is
achieved. \emph{Relative job size} allows to add a maximum of four
times the number of initially requested workers. The tests with the
lightweight simulator proved that the relative performance of the
frameworks could realistically be three or four times the performance
of the standard scheduling policy with a fixed number of workers.


\section{Conclusions and further work}

As we saw in the introduction, due to the diversity of processes that
are executed in a cluster, the need to share data between applications
and the scalability requirements of clusters, designing a scheduler is
a difficult task.

After a comparison of the different models of schedulers a group of
people at Google came up with the idea of Omega, a cluster scheduler
based on a shared-state approach that provides a good compromise
between simplicity and the fulfillment of the scheduling requirements
of Google's clusters. Omega can provide a good level of cluster
utilization due to every framework knowing the current state of
available resources for the whole cluster. It is well adapted for a
variety of framework's scheduling needs since every framework takes
care of doing his own scheduling and Omega only exposes the
availability of resources and protects against allocation
conflicts.

The experiments carried out with both a lightweight simulator and the
real traces of clusters show that Omega performs better than the
multi-path monolithic scheduler, even as the decision time of service
jobs grows and the possibility of conflicts increases two of the more
important constraints required by the context of google's
clusters. All of this with a design that is much simpler and much more
modular than the old monolithic scheduler. This simplicity and
modularity makes it way easier to extend the central scheduler and to
define new framework schedulers that follow use new strategies and
allows the scheduler to scale as the cluster and the number of jobs
increases. 

Besides, the design of Omega allows frameworks to dynamically increase
or decrease the resource consumption, for example to speed up the
execution of jobs when the cluster is under low usage. This has been
proved with the implementation of an elastic MapReduce scheduler that
modifies its scheduling strategy to benefit from resources available
in the cluster outperforming the standard MapReduce scheduler by a
factor of in between three and four. 


% Talk about limitation with scheduling working at too low level
% not benefiting from low level architecture (firmament model)

However even if Omega fits really well the context of Google clusters
it has its limitations. It doesn't work well when many jobs compete
frequently for access to the same resource (e.g many batch jobs want to run
on the same cluster node) or when cluster utilization is really high,
since the possibility for conflicts increases and Omega doesn't
provide a centralized way to coordinate the scheduling of different
frameworks. This lack of a centralized intelligence makes it difficult
to enforce global policies in the cluster or control \emph{rogue}
frameworks. For the moment Omega only provides global policies through
a simple mechanism for dealing with priorities and through post-facto
monitoring but this is one of the biggest axes of improvement for the
shared-state approach.

Another limitation that applies not only to Omega but to all the other
scheduling models explained in the article comes from the fact that
these models don't take into consideration low level details of the
machines that conform the cluster nor they provide support for 
adapting the scheduling decisions to this low level details. This is
partially because todays clusters are mainly form by sets of heterogeneous
machines, therefore it is more difficult to consider all the different
configurations of all the machines when scheduling.

The problem is that by not considering the low level details of
machines when scheduling has important considerations in the
performance of the tasks. We can take as an example the cost of
accesing memory for the applications running in a cluster.  Some
applications are more sensitive to the time spent accessing memory
because they are intrinsically more memory intensive. This is the kind
of application that benefits greatly from the different levels of
cache. However, having two applications of this type running at the
same time in the same machine degrades the performance significantly
because they both occupy the cache in turns and by doing that the
increase the number of misses. Today's models including Omega, don't
make this kind of optimization. It would be interesting to try to study 
how some of this changes affect the performance of the cluster. Some
work in this sense has already been done in \cite{_firmament_????} and luckily that
model could be applied to other kind of problems of inference
between applications not only due to memory inference but also due to
network, I/O or CPU interference.

\section{Bibliography}


\printbibliography


\end{document}
